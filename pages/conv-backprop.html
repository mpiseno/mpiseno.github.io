<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        .date {
            color: rgb(100, 100, 100);
            font-size: large;
        }
        .equation {
            padding-left: 20%;
            padding-right: 20%;
        }
        .figure {
            padding-left: 20%;
            padding-right: 20%;
            padding-top: 2em;
            padding-bottom: 2em;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
        }
        .figure img {
            width: 50%;
            max-width: 500px;
            height: auto;
        }
        .figure .caption {
            padding-top: 0px;
            font-size: small;
            color: rgb(100, 100, 100);
        }
    </style>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="/index.js"></script>
</head>
<body style="padding-left: 2%; padding-right: 2%;">
    <h1>Notes on Backpropagation in Convolutional Layers</h1>
    <div class="date">29 December 2020</div>
    <hr/>
    <p align="justify">
        This set of notes will discuss backpropagation in convolutional layers. We will derive the gradient of the loss with respect to the weights (kernel) and the input and discuss some interesting interpretations of these graidents. These notes were originially used for CS 7643: Deep Learning at Georgia Tech.
    </p>

    <!-- First we will formalize some notation. We assume that the layer has input $$\mathbf{X} \in \mathbb{R}^{N_{1} \times N_{2}}$$, a kernel $$\mathbf{w} \in \mathbb{R}^{k_{1} \times k_{2}}$$, and output $$\mathbf{y} \in \mathbb{R}^{N_{1} \times N_{2}}$$ (i.e. assume that we have sufficient padding to make the output the same dimension as the input). We also assume that the stride is 1. For simplcity, we are not including the channel dimension in the input or kernel (i.e. $$c = 1$$). Finally, $$\mathbf{X}$$ and $$\mathbf{y}$$ will be indexed by $$(r, c)$$ and $$\mathbf{w}$$ will be indexed by $$(a, b)$$. -->

    <h2>Preliminaries</h2>
    <p align="justify">
        First we will define some notation. We denote the input to the convolutional layer \( X \in \mathbb{R}^{H \times W} \), the output \( Y \in \mathbb{R}^{H \times W} \), and the parameters (i.e. the kernel) \( \Theta \in \mathbb{R}^{K \times K} \). For simplicity, we are assume the input and output have the same size, the kernel is square, and there is no channel dimension (i.e. there is one channel). We denote the scalar loss as \( L \).
    </p>
    <p align="justify">
        We also use the notation of differentials and gradients interchangiably. For example, \( \nabla_{\Theta} L = \frac{\partial L}{\partial \Theta} \in \mathbb{R}^{K^2} \) is a vector of partial derivatives the same size as the flattened \( \Theta \).
    </p>
    <p align="justify">
        Recall that in a convolutional layer, each activation in \( X \) is computed as a cross-correlation of some region of \( X \) with \( \Theta \).

        <div class="equation">
            \[ 
            \begin{equation}\label{eq:forward}
                y_{h,w} = \sum_{i=0}^{K-1} \sum_{j=0}^{K-1} x_{h+i,w+j} \theta_{i, j}
            \end{equation}
            \]
        </div>
    </p>
    <p align="justify">
        Also recall the basic API for the backward pass. The input is the upstream gradients from the next layer of the computation graph \( \frac{\partial L}{\partial Y} \) and the outputs are the gradients with respect to the parameters \( \frac{\partial L}{\partial \Theta} \) (used for gradient descent) and the input \( \frac{\partial L}{\partial X} \) (used for continuing the backward pass).
    </p>

    <h2>Computing \( \nabla_{\Theta} L \)</h2>
    <p align="justify">
        We will now derive \( \nabla_{\Theta} L = \frac{\partial L}{\partial \Theta} \in \mathbb{R}^{K^2} \). Consider computing the gradient one entry at a time by considering the scalar \( \frac{\partial L}{\partial \theta_{i,j}} \). Logically we know that the parameter \( \theta_{i,j} \) contributes to every activation in the output, so we much incorporate the derivative of each of these activations in our computation of \( \frac{\partial L}{\partial \theta_{i,j}} \). Using the chain rule we have:
        
        <div class="equation">
            \[
            \begin{align*}
                \frac{\partial L}{\partial \theta_{i,j}} &= \frac{\partial L}{\partial Y}\frac{\partial Y}{\partial \theta_{i,j}} \\
                &= \sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \frac{\partial L}{\partial y_{h,w}}\frac{\partial y_{h,w}}{\partial \theta_{i,j}}
            \end{align*}
            \]
        </div>
    </p>
    <p align="justify">
        We already have \( \frac{\partial L}{\partial y_{h,w}} \) from the upstream gradient. To compute \( \frac{\partial y_{h,w}}{\partial \theta_{i,j}} \), we can use Equation \( \ref{eq:forward} \).

        <div class="equation">
            \[
            \begin{align*}
                \frac{\partial}{\partial \theta_{i,j}} y_{h,w} &= \frac{\partial}{\partial \theta_{i,j}} \sum_{i'=0}^{K-1} \sum_{j'=0}^{K-1} x_{h+i',w+j'} \theta_{i', j'} \\
                &= x_{h+i,w+j}
            \end{align*}
            \]
        </div>
    </p>
    <p align="justify">
        This make sense because in the computation of \( y_{h,w} \), \( \theta_{i,j} \) is only used once when it is multiplied by \( x_{h+i,w+j} \). So for each parameter \( \theta_{i,j} \) we have:
        
        <div class="equation">
            \[
            \begin{equation}\label{eq:dLdTheta}
                \frac{\partial L}{\partial \theta_{i,j}} = \sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \frac{\partial L}{\partial y_{h,w}} x_{h+i,w+j}
            \end{equation}
            \]
        </div>
        
        This can be thought of as one "pixel" of the gradient \( \frac{\partial L}{\partial \Theta} \). An interesting interpretation of equation \( \ref{eq:dLdTheta} \) is that <b>the gradient \( \frac{\partial L}{\partial \Theta} \) is a cross-correlation between \( \frac{\partial L}{\partial Y} \) and the input \( X \)</b>. Actually, it is technically a cross-correlation using a padded version of \( X \) such that the output size is \( K \times K \).

        <div class="figure">
            <img src="/static/conv-backprop/dLdTheta.gif" style="max-width: 200px;">
            <p class="caption">Figure 1: Visualization of \( \frac{\partial L}{\partial \Theta} \) computation. </p>
        </div>
    </p>

    <h2>Computing \( \nabla_{X} L \)</h2>
    <p align="justify">
        Placeholder
    </p>

</body>
</html>
